{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching list of datasets from OpenML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9f/k42n6ytj32g13ppqb3kfpm880000gn/T/ipykernel_59263/3270224235.py:106: FutureWarning: Support for `output_format` of 'dict' will be removed in 0.15 and pandas dataframes will be returned instead. To ensure your code will continue to work, use `output_format`='dataframe'.\n",
      "  datasets_dict = openml.datasets.list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6024 datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6529ce84ab494bb3a70767fdb6adac6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing datasets:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 20/75 datasets processed\n",
      "  - Successful: 20\n",
      "  - Failed: 0\n",
      "  - Skipped (too few qualities): 0\n",
      "Progress: 40/75 datasets processed\n",
      "  - Successful: 40\n",
      "  - Failed: 0\n",
      "  - Skipped (too few qualities): 0\n",
      "Progress: 60/75 datasets processed\n",
      "  - Successful: 60\n",
      "  - Failed: 0\n",
      "  - Skipped (too few qualities): 0\n",
      "\n",
      "Completed downloading meta-features:\n",
      "  - Successful: 75 datasets\n",
      "  - Failed: 0 datasets\n",
      "  - Skipped (too few qualities): 0 datasets\n",
      "Meta-features saved to /Users/anukhayri/Desktop/mtlas/openml_datasets\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create a subfolder for storing datasets and meta-features\n",
    "base_folder = \"openml_datasets\"\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "# Define meta-feature categories (based on the paper)\n",
    "META_FEATURE_CATEGORIES = {\n",
    "    \"Simple\": [\n",
    "        \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\",\n",
    "        \"PercentageOfBinaryFeatures\", \"PercentageOfSymbolicFeatures\", \n",
    "        \"PercentageOfNumericFeatures\", \"PercentageOfMissingValues\",\n",
    "        \"PercentageOfInstancesWithMissingValues\", \n",
    "        \"MajorityClassPercentage\", \"MinorityClassPercentage\", \"Dimensionality\"\n",
    "    ],\n",
    "    \"Statistical\": [\n",
    "        \"MeanMeansOfNumericAtts\", \"MeanStdDevOfNumericAtts\", \n",
    "        \"MeanKurtosisOfNumericAtts\", \"MeanSkewnessOfNumericAtts\",\n",
    "        \"MinMeansOfNumericAtts\", \"MinStdDevOfNumericAtts\", \n",
    "        \"MinKurtosisOfNumericAtts\", \"MinSkewnessOfNumericAtts\",\n",
    "        \"MaxMeansOfNumericAtts\", \"MaxStdDevOfNumericAtts\", \n",
    "        \"MaxKurtosisOfNumericAtts\", \"MaxSkewnessOfNumericAtts\"\n",
    "    ],\n",
    "    \"Information_Theoretic\": [\n",
    "        \"ClassEntropy\", \"MeanAttributeEntropy\", \"MinAttributeEntropy\", \n",
    "        \"MaxAttributeEntropy\", \"MeanMutualInformation\", \"MinMutualInformation\", \n",
    "        \"MaxMutualInformation\", \"EquivalentNumberOfAtts\", \"MeanNoiseToSignalRatio\"\n",
    "    ],\n",
    "    \"Landmarking\": [\n",
    "        \"NaiveBayesAUC\", \"NaiveBayesErrRate\", \"kNN1NAUC\", \"kNN1NErrRate\",\n",
    "        \"DecisionStumpAUC\", \"DecisionStumpErrRate\", \"J48AUC\", \"J48ErrRate\",\n",
    "        \"RandomTreeDepth1AUC\", \"RandomTreeDepth1ErrRate\",\n",
    "        \"REPTreeDepth1AUC\", \"REPTreeDepth1ErrRate\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def get_dataset_metafeatures(dataset_id, verbose=False):\n",
    "    \"\"\"\n",
    "    Fetch meta-features for a specific dataset from OpenML\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get dataset\n",
    "        dataset = openml.datasets.get_dataset(dataset_id, download_data=False)\n",
    "        \n",
    "        # Get all qualities (meta-features)\n",
    "        qualities = dataset.qualities\n",
    "        \n",
    "        # Organize meta-features into categories\n",
    "        categorized_features = {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"name\": dataset.name,\n",
    "            \"Simple\": {},\n",
    "            \"Statistical\": {},\n",
    "            \"Information_Theoretic\": {},\n",
    "            \"Landmarking\": {},\n",
    "            \"Other\": {}\n",
    "        }\n",
    "        \n",
    "        # Add all qualities to appropriate categories\n",
    "        for key, value in qualities.items():\n",
    "            # Skip None values\n",
    "            if value is None:\n",
    "                continue\n",
    "                \n",
    "            # Check which category it belongs to\n",
    "            category_found = False\n",
    "            for category, features in META_FEATURE_CATEGORIES.items():\n",
    "                for feature in features:\n",
    "                    if feature in key:\n",
    "                        categorized_features[category][key] = value\n",
    "                        category_found = True\n",
    "                        break\n",
    "                if category_found:\n",
    "                    break\n",
    "            \n",
    "            # If not found in any category, put in Other\n",
    "            if not category_found:\n",
    "                categorized_features[\"Other\"][key] = value\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Found {sum(len(v) for k, v in categorized_features.items() if k not in ['dataset_id', 'name'])} meta-features for dataset {dataset_id}\")\n",
    "        \n",
    "        return categorized_features\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error fetching dataset {dataset_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# No custom meta-feature calculation needed\n",
    "\n",
    "def download_datasets_with_metafeatures(num_datasets=100, min_qualities=20, save_data=False):\n",
    "    \"\"\"\n",
    "    Download datasets from OpenML and save their meta-features\n",
    "    \n",
    "    Args:\n",
    "        num_datasets: Number of datasets to download (set high to get all)\n",
    "        min_qualities: Minimum number of qualities required for a dataset\n",
    "        save_data: Whether to save the actual dataset CSV\n",
    "    \"\"\"\n",
    "    print(\"Fetching list of datasets from OpenML...\")\n",
    "    datasets_dict = openml.datasets.list_datasets()\n",
    "    datasets = list(datasets_dict.items())\n",
    "    \n",
    "    print(f\"Found {len(datasets)} datasets\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, (dataset_id, dataset_info) in enumerate(tqdm(datasets[:num_datasets], desc=\"Processing datasets\")):\n",
    "        try:\n",
    "            # Fetch meta-features first (lightweight operation)\n",
    "            metafeatures = get_dataset_metafeatures(dataset_id)\n",
    "            \n",
    "            if metafeatures is None:\n",
    "                failed += 1\n",
    "                continue\n",
    "                \n",
    "            # Count total number of qualities\n",
    "            total_qualities = sum(len(v) for k, v in metafeatures.items() \n",
    "                               if k not in ['dataset_id', 'name'])\n",
    "            \n",
    "            # Skip if too few qualities\n",
    "            if total_qualities < min_qualities:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Create a valid folder name\n",
    "            dataset_name = metafeatures['name'].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            dataset_folder = os.path.join(base_folder, f\"{dataset_id}_{dataset_name}\")\n",
    "            os.makedirs(dataset_folder, exist_ok=True)\n",
    "            \n",
    "            # Save meta-features to JSON file\n",
    "            metafeatures_file = os.path.join(dataset_folder, \"metafeatures.json\")\n",
    "            with open(metafeatures_file, 'w') as f:\n",
    "                json.dump(metafeatures, f, indent=2)\n",
    "            \n",
    "            if save_data:\n",
    "                # Download the actual dataset (more heavyweight)\n",
    "                dataset = openml.datasets.get_dataset(dataset_id)\n",
    "                X, y, categorical_indicator, feature_names = dataset.get_data(\n",
    "                    target=dataset.default_target_attribute\n",
    "                )\n",
    "                \n",
    "                # Convert to pandas DataFrame\n",
    "                df = pd.DataFrame(X, columns=feature_names)\n",
    "                if y is not None:\n",
    "                    df['target'] = y\n",
    "                \n",
    "                # Save to CSV\n",
    "                csv_file = os.path.join(dataset_folder, f\"{dataset_name}.csv\")\n",
    "                df.to_csv(csv_file, index=False)\n",
    "            \n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset {dataset_id}: {str(e)}\")\n",
    "            failed += 1\n",
    "        \n",
    "        # Be nice to the API\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        # Print progress every 20 datasets\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Progress: {i+1}/{min(num_datasets, len(datasets))} datasets processed\")\n",
    "            print(f\"  - Successful: {successful}\")\n",
    "            print(f\"  - Failed: {failed}\")\n",
    "            print(f\"  - Skipped (too few qualities): {skipped}\")\n",
    "    \n",
    "    print(f\"\\nCompleted downloading meta-features:\")\n",
    "    print(f\"  - Successful: {successful} datasets\")\n",
    "    print(f\"  - Failed: {failed} datasets\")\n",
    "    print(f\"  - Skipped (too few qualities): {skipped} datasets\")\n",
    "    print(f\"Meta-features saved to {os.path.abspath(base_folder)}\")\n",
    "\n",
    "# Example usage in a Jupyter notebook:\n",
    "download_datasets_with_metafeatures(\n",
    "    num_datasets=75,  # Set to a large number to download many datasets\n",
    "    min_qualities=10,  # Minimum number of meta-features required\n",
    "    save_data=True    # Set to True to also save the actual CSV data\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
